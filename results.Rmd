---
title: "Analiza wydajności struktur pamięciowych baz danych"
author: "Mikołaj Mierzejewski, Kamil Osak, Filip Szóstak"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_float: yes
    theme: spacelab
    number_sections: yes
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set( echo  = FALSE)
library(dplyr)
library(ggplot2)
library(plotly)
```


```{r init, echo=FALSE}
draw_chart <- function(operation, trans = "identity") {
  data <- read.csv(paste("out/", operation, ".csv", sep=""), header=FALSE, col.names=c("database", "time", "size"))
  df <- data %>%
    mutate(database=factor(database, labels=c('Dictionary', 'List', 'T-Tree100', 'T-Tree500', 'T-Tree1000')), time=time/1000, size=size/1000) %>%
    group_by(database, size) %>%
    summarize(time=mean(time), .groups='drop')
  
  return(
    ggplot(df, aes(x=size, y=time, color=database, fill=database, shape=database)) + 
      geom_point() + 
      geom_smooth(formula = y ~ x, method = 'lm') + 
      labs(
        x="Database size [thousands of records]", 
        color="", 
        fillz="", 
        y="Operation time [s]", 
        title=paste("Memory structure effeciency for", operation, "operation.")) + 
      scale_y_continuous(trans=trans) + 
      scale_fill_discrete(name="Memory structure type:") +
      scale_color_discrete(name="Memory structure type:") +
      scale_shape_discrete(name="Memory structure type:")
   )
    
}
```

# Abstract

# Wprowadzenie

## Motywacje i opis problemu

## Wybrane struktury pamięciowe 

## Cel pracy 

# Implementacja struktur pamięciowych

## Lista

### Dodawanie

### Odczyt

### Odczyt sekwencyjny

### Aktualizacja

### Usuwanie

## Słownik

### Dodawanie

### Odczyt

### Odczyt sekwencyjny

### Aktualizacja

### Usuwanie

## T-Drzewo

### Dodawanie

### Odczyt

### Odczyt sekwencyjny

### Aktualizacja

### Usuwanie

# Metodologia badań

W tym rozdziale zostanie opisana obrana metodologia badań uwzględniająca poszczególne operacje, środowisko oraz sposób przeprowadzenia badań.

## Wybrane operacje

Do badań wybrano 6 różnych operacji:

- Dodawanie - do istniejącej bazy danych dodano *n* różnych nowych wartości o kolejnych wartościach indeksu, rozpoczynających się od wartości większej o `1` od największej obecnie istniejących. Pod każdy z kluczy generowano losowo dane osoby (imię, nazwisko, numer PESEL, adres mailowy, numer telefonu, wiek oraz adres).

- Odczyt - do odczytu wybierano każdorazowo losową wartość ze wszystkich wartości indeksu bazy danych.

- Odczyt sekwencyjny - do odczytu wybierano każdorazowo losową wartość początkową przedziału. Na jej podstawie każdorazowo dokonano `100` odczytów sekwencyjnych wartości kluczy. 

- Aktualizacja - do aktualizacji wybierano każdorazowo losową wartość ze wszystkich wartości indeksu bazy danych, a następnie dokonywano ponownego generowania danych osoby.

- Usuwanie - do usuwania wybrano każdorazowo losową wartość, od wartości minimalnej, do maksymalnej z bazy danych. Niektóre odczyty mogły próbować usunąć wartość nieistniejącą.

- Połączenie operacji - operacja składająca się z łańcucha 600 operacji odczytu z całej bazy danych, `600` operacji dodania wartości o nowych indeksach wzorem operacji dodania oraz 600 operacji usunięcia dodanych przed chwilą danych.

## Przeprowadzone badania 

W celu uzyskania miarodajnych wyników, objęto poniższą metodologie badań.

Każda z operacji została wykonana `3000` razy pod rząd, w celu uniknięcia tzw. outlayerów, mierząc czas wykonania zbioru operacji. Pozwoliło to osiągnąć uśrednione czasy dla każdej z operacji. W czas operacji jest również wliczony losowy wybór wartości z przedziału zależnego od wielkości danych.

Sekwencje operacji zostały uruchomione na bazach danych z różną, początkową ilością danych. zaczynając na `10,000` rekordach, kończąc na `100,000` z krokiem `10,000`.

W celu zminimalizowania wpływu usług uruchamianych przez system w tle niezależnie od użytkownika, generując outlayery, cały proces został wykonany `3` razy, pozwalając na uśrednienie wyników badań.

## Opis środowiska

Środowisko do przeprowadzenia badań stanowi komputer wyposażony w:

- Procesor Ryzen 5 3600
- 16 GB pamięci DDR4 o taktowaniu 3200 MHz
- System Windows 11 w wersji `21H2`
- Python w wersji `3.11.1`

W ramach przeprowadzanych badań, nie dokonywano żadnych operacji na wyżej wymienionym systemie.

# Wyniki badań

W tej sekcji zawiera się wizualizacja oraz opis wyników przeprowadzonych badań.

## Dodawanie danych
```{r create}
ggplotly(draw_chart("create", "log10"))
```

Zauważyć można, że w procesie dodawania najwolniejsza jest baza danych oparta na liście, gdzie jej złożoność rośnie mniej-więcej liniowo. 

Następna w kolejności jak chodzi o prędkość jest baza oparta na t-drzewie dla węzła o wielkości 100, którego złożoność rożnie mniej więcej liniowo 
W przypadku dwóch pozostałych baz opartych na t-drzewach zauważyć można, że rosną one bardzo powoli. 

Najszybszą operację dodawania posiada baza oparta na słowniku, której to czas odczytu jest stały.

## Odczyt losowy danych
```{r read}
ggplotly(draw_chart("read", "log10"))
```

Operacje odczytu losowego prezentują się podobie do operacji dodawania danych. 

Najwolniejszy odczyt posiada struktura listowa, następnie struktury oparte na t-drzewie, a najszybsza jest struktura oparta na słowniku, która to jest jedyna niezależna od wielkości bazy danych.

## Odczyt sekwencyjny danych
```{r read_range}
ggplotly(draw_chart("read_range", "log10"))
```

Operacja odczytu sekwencyjnego na bazie opartej na słowniku jest najwolniejsza. Nieznacznie szybsza jest ta oparta na liście oraz na t-drzewie z węzłem o wielkości 100.

Znacznie szybsze są struktury oparte na t-drzewie z węzłami o wielkościach 500 oraz 1000.

Wszystkie struktury wykazały tutaj złożoność liniową.

## Aktualizacja danych
```{r update}
ggplotly(draw_chart("update", "log10"))
```

Zdecydowanie najwolniejszą strukturą do tej operacji jest lista, której to złożoność rośnie liniowo.

Wszystkie pozostałe struktury wykonują tą operację znacznie szybciej. Jednak warto zauważyć, że poza listą, t-drzewo z węzłem o wielkości 100 widocznie zależy od wielkości bazy danych. 

## Usuwanie danych
```{r delete}
ggplotly(draw_chart("delete", "log10"))
```

Usuwanie danych z listy zajmuje najwięcej czasu i rośnie ono liniowo.

Następna jest baza oparta na t-drzewie o wielkości 100, 1000 oraz 500, które rosną logarytmicznie. 

Najszybsze jest operowanie na strukturze słownika, które rośnie nieznacznie w zależności od wielkości bazy danych.

## Połączenie operacji
```{r mixture}
ggplotly(draw_chart("mixture", "log10"))
```

Połączenie operacji prezentuje się analogicznie do wykresu operacji dodawania, gdzie operowanie na liście zajmuje najdłużej, następnie jest operowanie na t-drzewie o węźle z wielkością 100, 1000, 500 oraz na słowniku.

# Zakończenie

## Podsumowanie prac i wnioski

# Literatura

